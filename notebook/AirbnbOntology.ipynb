{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1730dd7e",
   "metadata": {},
   "source": [
    "<b>Database Project, Populate the Airbnb Ontology</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17210dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from rdflib import Graph, Literal, RDF, RDFS, URIRef, Namespace\n",
    "from rdflib.namespace import XSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ba666f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"./data/\"\n",
    "output_dir = \"./outputs/\"\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# path to dataset\n",
    "listingsUrl = os.path.join(data_dir, \"listings.csv\")\n",
    "reviewsUrl = os.path.join(data_dir, \"reviews.csv\")\n",
    "\n",
    "# saving folder\n",
    "savePath =  output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc52daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removes any special character and converts to lowercase\n",
    "def clean_string(dirty_string: str):\n",
    "    return re.sub('[^A-Za-z0-9]+', '', dirty_string).lower()\n",
    "\n",
    "# removes any special character from a string number\n",
    "def clean_number(dirty_number: str):\n",
    "    return re.sub('[^\\.0-9]+', '', dirty_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a430ed10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the ontology namespace because it is not known by rdflib\n",
    "AO = Namespace(\"http://www.dei.unipd.it/database2/airbnbOntology#\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f40b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = pd.read_csv(reviewsUrl, sep=',', index_col='id')\n",
    "listings = pd.read_csv(listingsUrl, sep=',', index_col='id')\n",
    "\n",
    "# droping the whole column since we don't use it\n",
    "reviews.drop(['comments'], axis=1, inplace=True) \n",
    "# droping 5 rows that have missing values based on reviewr_name\n",
    "reviews.dropna(subset=['reviewer_name'], axis=0, inplace=True) \n",
    "# droping duplicate reviewer_id's and keeping only distinct values\n",
    "reviews.drop_duplicates(subset=['reviewer_id'], inplace=True)\n",
    "# dropping the rows in which reviewer names have special characters for more readability\n",
    "reviews.drop(reviews[reviews.reviewer_name.str.contains(r'[^0-9a-zA-Z]')].index, inplace = True)\n",
    "\n",
    "print(\"----------Reviews Done----------\")\n",
    "\n",
    "# keeping only the columns that we are using in our graph\n",
    "listings = listings[['host_id', 'host_name', 'host_since', 'host_listings_count', \n",
    "                     'neighbourhood_cleansed', 'neighbourhood_group_cleansed', 'room_type', \n",
    "                     'price', 'minimum_nights', 'availability_365', 'number_of_reviews', 'review_scores_rating']]\n",
    "\n",
    "# droping 50 rows that have missing values based on host_name\n",
    "listings.dropna(subset=['host_name'], axis=0, inplace=True)  \n",
    "# filling in the 8362 missing values of ratings with the average of all other ratings\n",
    "listings['review_scores_rating'].fillna(listings[\"review_scores_rating\"].mean(), inplace=True)\n",
    "\n",
    "print(\"----------Listings Done----------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad669b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "g = Graph()\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"ao\", AO)\n",
    "\n",
    "inserted_boroughs = []\n",
    "# iterate over the listings dataset\n",
    "for index, row in listings.iterrows():\n",
    "    if not row['neighbourhood_group_cleansed'] in inserted_boroughs:\n",
    "        inserted_boroughs.append(row['neighbourhood_group_cleansed'])\n",
    "        # the node has the namespace + the neighbourhood_group_cleansed name itself as URI\n",
    "        idU = clean_string(row['neighbourhood_group_cleansed'])\n",
    "        Borough = URIRef(AO[idU])\n",
    "        \n",
    "        # add triples\n",
    "        g.add((Borough, RDF.type, AO.Borough))\n",
    "        g.add((Borough, AO['boroughName'], Literal(row['neighbourhood_group_cleansed'], datatype=XSD.string)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8364ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# print all the data in the turtle format\n",
    "print(\"--- saving serialization ---\")\n",
    "with open(savePath + 'borough.ttl', 'w') as file:\n",
    "    file.write(g.serialize(format='turtle'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2bfda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "g = Graph()\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"ao\", AO)\n",
    "\n",
    "inserted_areas = []\n",
    "# iterate over the listings dataset\n",
    "for index, row in listings.iterrows():\n",
    "    if not row['neighbourhood_cleansed'] in inserted_areas:\n",
    "        inserted_areas.append(row['neighbourhood_cleansed'])\n",
    "        # the node has the namespace + the neighbourhood_cleansed name itself as URI\n",
    "        idU = clean_string(row['neighbourhood_cleansed'])\n",
    "        Area = URIRef(AO[idU])\n",
    "        Borough = URIRef(AO[clean_string(str(row['neighbourhood_group_cleansed']))])\n",
    "\n",
    "        # add triples\n",
    "        g.add((Area, RDF.type, AO.Area))\n",
    "        g.add((Area, AO['areaName'], Literal(row['neighbourhood_cleansed'], datatype=XSD.string)))\n",
    "        g.add((Area, RDFS.subClassOf, Borough))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427c14c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# print all the data in the turtle format\n",
    "print(\"--- saving serialization ---\")\n",
    "with open(savePath + 'area.ttl', 'w') as file:\n",
    "    file.write(g.serialize(format='turtle'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21728f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "g = Graph()\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"ao\", AO)\n",
    "\n",
    "inserted_hosts = []\n",
    "# iterate over the listings dataset\n",
    "for index, row in listings.iterrows():\n",
    "    if not row['host_id'] in inserted_hosts:\n",
    "        inserted_hosts.append(row['host_id'])\n",
    "        # the node has the namespace + the host_id as URI\n",
    "        idU = \"host\"+str(index)\n",
    "        Host = URIRef(AO[idU])\n",
    "\n",
    "        # add triples \n",
    "        g.add((Host, RDF.type, AO.Host))\n",
    "        g.add((Host, AO['personName'], Literal(row['host_name'], datatype=XSD.string)))\n",
    "        g.add((Host, AO['hostListingsCount'], Literal(int(row['host_listings_count']), datatype=XSD.int)))\n",
    "        g.add((Host, AO['hostSince'], Literal(row['host_since'], datatype=XSD.string)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608a2020",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# print all the data in the turtle format\n",
    "print(\"--- saving serialization ---\")\n",
    "with open(savePath + 'host.ttl', 'w', encoding=\"utf-8\") as file:\n",
    "    file.write(g.serialize(format='turtle'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83cccdbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "g = Graph()\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"ao\", AO)\n",
    "\n",
    "inserted_listing_types = []\n",
    "\n",
    "# iterate over the listings dataset\n",
    "for index, row in listings.iterrows():\n",
    "    if not row['room_type'] in inserted_listing_types:\n",
    "        inserted_listing_types.append(row['room_type'])\n",
    "        # the node has the namespace + the room_type name itself as URI\n",
    "        idU = clean_string(row['room_type'])\n",
    "        ListingType = URIRef(AO[idU])\n",
    "\n",
    "        # add triples\n",
    "        g.add((ListingType, RDF.type, AO.ListingType))\n",
    "        g.add((ListingType, AO['listingTypeName'], Literal(row['room_type'], datatype=XSD.string)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f27f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# print all the data in the turtle format\n",
    "print(\"--- saving serialization ---\")\n",
    "with open(savePath + 'listing_type.ttl', 'w') as file:\n",
    "    file.write(g.serialize(format='turtle'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a129946d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "g = Graph()\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"ao\", AO)\n",
    "\n",
    "# iterate over the listings dataset\n",
    "for index, row in listings.iterrows():\n",
    "    # the node has the namespace + the listing_id as URI\n",
    "    idU1 = \"listing\"+str(index)\n",
    "    idU2 = \"host\"+str(row['host_id'])\n",
    "    Listing = URIRef(AO[idU1])\n",
    "    ListingType = URIRef(AO[clean_string(str(row['room_type']))])\n",
    "    Area = URIRef(AO[clean_string(str(row['neighbourhood_cleansed']))])\n",
    "    Host = URIRef(AO[idU2])\n",
    "\n",
    "    # add triples\n",
    "    g.add((Listing, RDF.type, AO.Listing))\n",
    "    g.add((Listing, AO['price'], Literal(clean_number(row['price']), datatype=XSD.float)))\n",
    "    g.add((Listing, AO['minNumNights'], Literal(row['minimum_nights'], datatype=XSD.int)))\n",
    "    g.add((Listing, AO['numReviews'], Literal(row['number_of_reviews'], datatype=XSD.int)))\n",
    "    g.add((Listing, AO['reviewRating'], Literal(row['review_scores_rating'], datatype=XSD.float)))\n",
    "    g.add((Listing, AO['availability365'], Literal(row['availability_365'], datatype=XSD.int)))\n",
    "    g.add((Listing, AO['locatedIn'], Area))\n",
    "    g.add((Listing, AO['hasType'], ListingType))\n",
    "    g.add((Listing, AO['belongsTo'], Host))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352943b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# print all the data in the turtle format\n",
    "print(\"--- saving serialization ---\")\n",
    "with open(savePath + 'listing.ttl', 'w') as file:\n",
    "    file.write(g.serialize(format='turtle'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0fba34",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "g = Graph()\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"ao\", AO)\n",
    "\n",
    "# iterate over the reviews dataset\n",
    "for index, row in reviews.iterrows():    \n",
    "    # the node has the namespace + the id as URI\n",
    "    idU1 = \"review\"+str(index)\n",
    "    idU2 = \"listing\"+str(row['listing_id'])\n",
    "    Review = URIRef(AO[idU1])\n",
    "    Listing = URIRef(AO[idU2])\n",
    "    Guest = URIRef(AO[str(row['reviewer_id'])])\n",
    "    \n",
    "    # add triples\n",
    "    g.add((Review, RDF.type, AO.Review))\n",
    "    g.add((Review, AO['date'], Literal(row['date'], datatype=XSD.string)))\n",
    "    g.add((Review, AO['referredTo'], Listing))\n",
    "    g.add((Review, AO['writtenBy'], Guest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b99509",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# print all the data in the turtle format\n",
    "print(\"--- saving serialization ---\")\n",
    "with open(savePath + 'review.ttl', 'w') as file:\n",
    "    file.write(g.serialize(format='turtle'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf54e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "g = Graph()\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"ao\", AO)\n",
    "\n",
    "\n",
    "# iterate over the reviews dataset\n",
    "for index, row in reviews.iterrows():\n",
    "    # the node has the namespace + the reviewer_id as URI\n",
    "    idU = \"guest\"+str(row['reviewer_id'])\n",
    "    Guest = URIRef(AO[idU])\n",
    "\n",
    "    # add triples\n",
    "    g.add((Guest, RDF.type, AO.Guest))\n",
    "    g.add((Guest, AO['personName'], Literal(row['reviewer_name'], datatype=XSD.string)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ec582b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# print all the data in the turtle format\n",
    "print(\"--- saving serialization ---\")\n",
    "with open(savePath + 'guest.ttl', 'w', encoding=\"utf-8\") as file:\n",
    "    file.write(g.serialize(format='turtle'))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b8b615314ffcaae1f195bdbe005bea144a66f2fd8009609bbd777615eaa78fa8"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
